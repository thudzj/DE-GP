{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DE-GP- Regression Example in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "# for NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# for NNGP\n",
    "import neural_tangents as nt\n",
    "from neural_tangents import stax\n",
    "\n",
    "# for HMC and VI\n",
    "import theano\n",
    "floatX = theano.config.floatX\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from utils_degp import ParallelPriorLinear, ParallelPriorConv2d, gp_sample_and_estimate_kl\n",
    "\n",
    "#prepare\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "np.random.seed(101)\n",
    "random.seed(101)\n",
    "torch.manual_seed(101)\n",
    "\n",
    "# general settings\n",
    "dataset = 1\n",
    "if dataset == 2:\n",
    "    n_hidden = 1024 \t# no. hidden units in NN\n",
    "    n_layer = 3\n",
    "    data_noise = 0.01\n",
    "    xlim = [-3, 3]\n",
    "    ylim = [-3, 3]\n",
    "elif dataset == 1:\n",
    "    xlim = [-3, 3]\n",
    "    ylim = [-3, 3]\n",
    "else:\n",
    "    n_hidden = 512 \t# no. hidden units in NN\n",
    "    n_layer = 3\n",
    "    data_noise = 0.001 # estimated noise variance    \n",
    "    xlim = [-1.7, 1.5]\n",
    "    ylim = [-3, 1.5]\n",
    "\n",
    "\n",
    "activation_fn = 'relu' # relu erf\n",
    "# W_var=2. \n",
    "# b_var=1.\n",
    "# W1_var = W_var\n",
    "# b1_var = b_var\n",
    "# W2_var = W_var/n_hidden\n",
    "# b2_var = b_var\n",
    "\n",
    "# optimisation options for NN\n",
    "# epochs = 300\n",
    "# l_rate = 0.003 \t\t# learning rate\n",
    "\n",
    "# for ensemble\n",
    "n_ensemble = 50\t# no. NNs in ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "class Erf(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Erf, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.erf()\n",
    "\n",
    "class Idt(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Idt, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "def fn_make_data(dataset = 0):\n",
    "    # create some data\n",
    "    if dataset == 0:\n",
    "        x_train = np.atleast_2d([1., 4.5, 5.1, 6., 8., 9.]).T\n",
    "        x_train = x_train/5. - 1\n",
    "        y_train = x_train * np.sin(x_train*5.) + np.random.normal(loc=0.,scale=0.2, size=x_train.shape)\n",
    "        y_train = y_train - y_train.mean()\n",
    "    elif dataset == 2:\n",
    "        x_train = np.atleast_2d(np.concatenate([np.linspace(-1.5,-0.6,10), np.linspace(.6,1.5,10)])).T\n",
    "        y_train =  x_train**3/4. + np.random.normal(loc=0.,scale=0.2, size=x_train.shape)\n",
    "        y_train = y_train - y_train.mean()\n",
    "    elif dataset == 1:\n",
    "        np.random.seed(1000)\n",
    "        x_train = np.atleast_2d(np.linspace(-1.5,1.5,8)).T\n",
    "        y_train =  np.sin(x_train*2) + np.random.normal(loc=0.,scale=0.2, size=x_train.shape)\n",
    "        y_train[-1] = y_train[-1]-1.2\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # create validation data - here we'll just a 1-d grid\n",
    "    x_val = np.atleast_2d(np.linspace(-3, 3, 100)).T\n",
    "    y_val = np.expand_dims(x_val[:,0],1) # just dummy data\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "def fn_make_NN(activation_fn, n_layer=1, n_hidden=100, dropout=0.):\n",
    "    D_in, D_out = 1, 1 # input and output dimension\n",
    "    \n",
    "    if activation_fn == 'relu':\n",
    "        mid_act = torch.nn.ReLU()\n",
    "    elif activation_fn == 'erf':\n",
    "        mid_act = Erf()\n",
    "    elif activation_fn == 'idt':\n",
    "        mid_act = Idt()\n",
    "    \n",
    "    layers = [torch.nn.Linear(D_in, n_hidden),\n",
    "        mid_act,\n",
    "        torch.nn.Dropout(dropout),\n",
    "        torch.nn.Linear(n_hidden, D_out, bias=False)]\n",
    "    for _ in range(n_layer - 1):\n",
    "        layers.insert(3, torch.nn.Linear(n_hidden, n_hidden))\n",
    "        layers.insert(4, mid_act)\n",
    "        layers.insert(5, torch.nn.Dropout(dropout))\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "def init_NN(model, W1_var, b1_var, W2_var, b2_var):\n",
    "    # initialise weights\n",
    "    model[0].weight.data.normal_(0.0, np.sqrt(W1_var))\n",
    "    if model[0].bias is not None:\n",
    "        model[0].bias.data.normal_(0.0, np.sqrt(b1_var))\n",
    "    for n, p in model[1:].named_parameters():\n",
    "        if 'bias' in n:\n",
    "            p.data.normal_(0.0, np.sqrt(b2_var))\n",
    "        else:\n",
    "            p.data.normal_(0.0, np.sqrt(W2_var))\n",
    "    \n",
    "def fn_make_prior_NN(activation_fn, n_ensembles, W_var, b_var, n_hidden=100):\n",
    "    D_in, D_out = 1, 1 # input and output dimension\n",
    "    \n",
    "    if activation_fn == 'relu':\n",
    "        mid_act = torch.nn.ReLU()\n",
    "    elif activation_fn == 'erf':\n",
    "        mid_act = Erf()\n",
    "    elif activation_fn == 'idt':\n",
    "        mid_act = Idt()\n",
    "        \n",
    "    layers = []\n",
    "    for i, n_ensemble in enumerate(n_ensembles):\n",
    "        in_features, out_features, bias = n_hidden, n_hidden, True\n",
    "        if i == 0:\n",
    "            in_features = D_in\n",
    "        layers.append(ParallelPriorLinear(n_ensemble, in_features, out_features, W_var=W_var, b_var=b_var, bias=bias))\n",
    "        layers.append(mid_act)\n",
    "\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "def fn_predict_ensemble(NNs, x_test):\n",
    "    ''' fn to predict given a list of NNs (an ensemble)''' \n",
    "    y_preds = []\n",
    "    with torch.no_grad():\n",
    "        for m in range(len(NNs)):\n",
    "            y_preds.append(NNs[m](torch.tensor(x_test).float()).data.numpy())\n",
    "    y_preds = np.array(y_preds)\n",
    "\n",
    "    y_preds_mu = np.mean(y_preds,axis=0)\n",
    "    y_preds_std = np.std(y_preds,axis=0)\n",
    "\n",
    "    return y_preds, y_preds_mu, y_preds_std\n",
    "\n",
    "def plot(x_test, y_preds, y_preds_mu, y_preds_std, data_noise, not_apply_data_noise=False,\n",
    "         show_preds=True, show_mu_std=False, show_trains=False, title=None, xlim=[-3, 3], ylim=[-3, 3]):\n",
    "\n",
    "    # plot predictive distribution\n",
    "    # if not not_apply_data_noise and y_preds_std is not None:\n",
    "        # y_preds_std = np.sqrt(np.square(y_preds_std) + data_noise) \n",
    "    if not_apply_data_noise and y_preds_std is not None:\n",
    "        y_preds_std = np.sqrt(np.square(y_preds_std) - data_noise) \n",
    "    fig = plt.figure(figsize=(5, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    if show_preds:\n",
    "        for m in range(0,n_ensemble):\n",
    "            ax.plot(x_test, y_preds[m], 'k')\n",
    "    if show_mu_std:\n",
    "        ax.plot(x_test, y_preds_mu, 'b-', linewidth=2.,label=u'Prediction')\n",
    "        ax.plot(x_test, y_preds_mu + 2 * y_preds_std, 'b', linewidth=0.5)\n",
    "        ax.plot(x_test, y_preds_mu - 2 * y_preds_std, 'b', linewidth=0.5)\n",
    "        ax.plot(x_test, y_preds_mu + 1 * y_preds_std, 'b', linewidth=0.5)\n",
    "        ax.plot(x_test, y_preds_mu - 1 * y_preds_std, 'b', linewidth=0.5)\n",
    "        ax.fill(np.concatenate([x_test, x_test[::-1]]),\n",
    "                 np.concatenate([y_preds_mu - 2 * y_preds_std,\n",
    "                                (y_preds_mu + 2 * y_preds_std)[::-1]]),\n",
    "                 alpha=1, fc='lightskyblue', ec='None')\n",
    "        ax.fill(np.concatenate([x_test, x_test[::-1]]),\n",
    "                 np.concatenate([y_preds_mu - 1 * y_preds_std,\n",
    "                                (y_preds_mu + 1 * y_preds_std)[::-1]]),\n",
    "                 alpha=1, fc='deepskyblue', ec='None')\n",
    "    if show_trains:\n",
    "        ax.plot(x_train[:,0], y_train, 'r.', markersize=14,\n",
    "                markeredgecolor='k',markeredgewidth=0.5)\n",
    "\n",
    "    ax.set_ylim(*ylim)\n",
    "    ax.set_xlim(*xlim)\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    if title:\n",
    "        fig.savefig(title +'.pdf', format='pdf', dpi=1000, bbox_inches='tight')\n",
    "    else:\n",
    "        fig.show()\n",
    "\n",
    "# create some data\n",
    "x_train, y_train, x_test, y_test = fn_make_data(dataset)\n",
    "plot = partial(plot, xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marg_log_like [[-53.46266427]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zhijie/env3/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marg_log_like [[-22.82699387]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zhijie/env3/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marg_log_like [[-18.16283912]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zhijie/env3/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marg_log_like [[-16.31607881]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zhijie/env3/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marg_log_like [[-15.32745927]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zhijie/env3/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: covariance is not positive-semidefinite.\n"
     ]
    }
   ],
   "source": [
    "def nngp(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "         W1_var, b1_var, W2_var, b2_var, n_layer, n_hidden, empirical=False):\n",
    "    if n_layer == 0:\n",
    "        layers = [stax.Dense(1, W_std=math.sqrt(W1_var), b_std=math.sqrt(b1_var), parameterization='standard')]\n",
    "    else:\n",
    "        layers = [stax.Dense(n_hidden, W_std=math.sqrt(W1_var), b_std=math.sqrt(b1_var), parameterization='standard'),\n",
    "            stax.Relu() if activation_fn == 'relu' else stax.Erf(),\n",
    "            stax.Dense(1, W_std=math.sqrt(W2_var*n_hidden), b_std=0, parameterization='standard')\n",
    "        ]\n",
    "        for _ in range(n_layer - 1):\n",
    "            layers.insert(2, stax.Dense(n_hidden, W_std=math.sqrt(W2_var*n_hidden), b_std=math.sqrt(b2_var), \n",
    "                                        parameterization='standard'))\n",
    "            layers.insert(3, stax.Relu() if activation_fn == 'relu' else stax.Erf())\n",
    "    init_fn, apply_fn, kernel_fn = stax.serial(*layers)\n",
    "    \n",
    "    if empirical:\n",
    "        from jax import random\n",
    "        key1, key2, key3 = random.split(random.PRNGKey(1), 3)\n",
    "        _, params = init_fn(key3, x_train.shape)\n",
    "        kernel_fn = nt.empirical_kernel_fn(apply_fn, trace_axes=(-1,), vmap_axes=0, implementation=1)\n",
    "        cov_dd = kernel_fn(x_train, None, 'nngp', params) + np.identity(x_train.shape[0])*data_noise\n",
    "        cov_xd = kernel_fn(x_test, x_train, 'nngp', params)\n",
    "        cov_xx = kernel_fn(x_test, None, 'nngp', params)\n",
    "    else:\n",
    "        cov_dd = kernel_fn(x_train, None, 'nngp') + np.identity(x_train.shape[0])*data_noise\n",
    "        cov_xd = kernel_fn(x_test, x_train, 'nngp')\n",
    "        cov_xx = kernel_fn(x_test, None, 'nngp')\n",
    "\n",
    "    L = np.linalg.cholesky(cov_dd)\n",
    "    alpha = np.linalg.solve(L.T,np.linalg.solve(L,y_train))\n",
    "    y_pred_mu = np.matmul(cov_xd,alpha)\n",
    "    v = np.linalg.solve(L,cov_xd.T)\n",
    "    cov_pred = cov_xx - np.matmul(v.T,v)\n",
    "\n",
    "    y_pred_var = np.atleast_2d(np.diag(cov_pred) + data_noise).T\n",
    "    y_pred_std = np.sqrt(y_pred_var)\n",
    "\n",
    "    marg_log_like = - np.matmul(y_train.T,alpha)/2 - np.sum(np.log(np.diag(L))) - x_train.shape[0]*np.log(2*np.pi)/2\n",
    "    print('marg_log_like', marg_log_like)\n",
    "    \n",
    "    # visualize priors\n",
    "    y_samples_prior = np.random.multivariate_normal(\n",
    "        np.zeros(x_test.shape[0]), cov_xx, n_ensemble) # mean, covariance, size\n",
    "    plot(x_test, y_samples_prior, None, None, data_noise, not_apply_data_noise=True,\n",
    "         show_preds=True, show_mu_std=False, show_trains=False)\n",
    "    \n",
    "    # visualize posteriors\n",
    "    plot(x_test, None, y_pred_mu, y_pred_std, data_noise, not_apply_data_noise=True,\n",
    "         show_preds=False, show_mu_std=True, show_trains=True, title='nngp_l{}_h{}'.format(n_layer, n_hidden))\n",
    "\n",
    "W_var=2\n",
    "b_var=1\n",
    "data_noise = 0.001 if dataset == 0 else 0.04\n",
    "n_hidden = 64\n",
    "W1_var = W_var\n",
    "b1_var = b_var\n",
    "W2_var = W_var/n_hidden\n",
    "b2_var = b_var\n",
    "nngp(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "     W_var, b1_var, W2_var, b2_var, 0, n_hidden, empirical=False)\n",
    "nngp(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "     W1_var, b1_var, W2_var, b2_var, 1, n_hidden, empirical=False)\n",
    "nngp(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "     W1_var, b1_var, W2_var, b2_var, 2, n_hidden, empirical=False)\n",
    "nngp(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "     W1_var, b1_var, W2_var, b2_var, 3, n_hidden, empirical=False)\n",
    "nngp(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "     W1_var, b1_var, W2_var, b2_var, 4, n_hidden, empirical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function gp_ensemble.<locals>.<lambda> at 0x7f04c82e99d8>\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:25<00:00, 39.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002506978511810303 0.0007129592972715628\n",
      "4.3590068817138675e-05 1.1118593879904329e-05\n",
      "0.006493837833404541 0.005265843451204032\n",
      "0.014219050407409667 0.0054885115609150876\n",
      "0.02326345682144165 0.009924810675785269\n",
      "Sequential(\n",
      "  (0): ParallelPriorLinear()\n",
      "  (1): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=64, out_features=1, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:39<00:00, 25.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005866787433624268 0.0012713199328617637\n",
      "0.0017871904373168946 0.0018499819861189622\n",
      "0.007895221710205078 0.0074461721590658204\n",
      "0.01886669874191284 0.0069891287479014455\n",
      "0.03441589832305908 0.014676940941151786\n",
      "Sequential(\n",
      "  (0): ParallelPriorLinear()\n",
      "  (1): ReLU()\n",
      "  (2): ParallelPriorLinear()\n",
      "  (3): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.0, inplace=False)\n",
      "  (6): Linear(in_features=128, out_features=1, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:17<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019406223297119142 0.017512639487244845\n",
      "0.004392557144165039 0.0019010815741191355\n",
      "0.004279713630676269 0.0029019364146233247\n",
      "0.035229172706604 0.020207002814921264\n",
      "0.06330766677856445 0.02845187374785738\n",
      "Sequential(\n",
      "  (0): ParallelPriorLinear()\n",
      "  (1): ReLU()\n",
      "  (2): ParallelPriorLinear()\n",
      "  (3): ReLU()\n",
      "  (4): ParallelPriorLinear()\n",
      "  (5): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.0, inplace=False)\n",
      "  (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Dropout(p=0.0, inplace=False)\n",
      "  (9): Linear(in_features=256, out_features=1, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:43<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024322364330291748 0.01349543997936938\n",
      "0.021582958698272706 0.006516483816349962\n",
      "0.0051937270164489745 0.004704726102526045\n",
      "0.10525599718093873 0.02833849897227917\n",
      "0.15635504722595214 0.03696555275043859\n"
     ]
    }
   ],
   "source": [
    "def gp_ensemble(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "                W_var, b_var, W1_var, b1_var, W2_var, b2_var, n_layer, n_hidden, kl_weight=1., \n",
    "                n_ensembles_ref=None, n_hidden_ref=None, ip=None):\n",
    "    x = torch.tensor(x_train).float()\n",
    "    y = torch.tensor(y_train).float()\n",
    "    # set up loss\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    if n_ensembles_ref is None:\n",
    "        n_ensembles_ref = [n_ensemble,] * (n_layer)\n",
    "    elif isinstance(n_ensembles_ref, int):\n",
    "        n_ensembles_ref = [n_ensembles_ref,] * (n_layer)\n",
    "    if n_hidden_ref is None:\n",
    "        n_hidden_ref = n_hidden\n",
    "    \n",
    "    if n_layer <= 0:\n",
    "        NN_ref = lambda x: x[:, None, :]\n",
    "    else:\n",
    "        NN_ref = fn_make_prior_NN(activation_fn, n_ensembles_ref, W_var, b_var, n_hidden_ref)\n",
    "    \n",
    "    print(NN_ref)\n",
    "    \n",
    "    # create the NNs\n",
    "    NNs, params = [], []\n",
    "    for m in range(n_ensemble):\n",
    "        if n_layer <= 0:\n",
    "            NNs.append(torch.nn.Sequential(torch.nn.Linear(1, 1)))\n",
    "        else:\n",
    "            NNs.append(fn_make_NN(activation_fn, n_layer, n_hidden))\n",
    "            \n",
    "        init_NN(NNs[-1], W1_var, b1_var, W2_var, b2_var)\n",
    "        for p in NNs[m].parameters():\n",
    "            params.append(p)\n",
    "    print(NNs[-1])\n",
    "    \n",
    "    if opt == 'adam':\n",
    "        optimizer = torch.optim.Adam(params, lr=l_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(params, lr=l_rate, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "    NNs_hist_train=[]\n",
    "    kls = []\n",
    "    \n",
    "    t1, t2, t3, t4 = [], [], [], []\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        for layer in range(n_layer+1):\n",
    "            tmp = 0\n",
    "            for m in range(n_ensemble):\n",
    "                tmp += NNs[m][layer * 3].weight.data.view(-1).norm() ** 2\n",
    "        if ip is None:\n",
    "            input = x\n",
    "        elif ip == 'uniform':\n",
    "            input = torch.cat([x, torch.empty(x.shape[0], 1).uniform_(-2, 2)], 0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        last_time = -time.time()\n",
    "        outputs = [NNs[m](input) for m in range(n_ensemble)]\n",
    "        t1.append(time.time() + last_time); last_time = -time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_preds_ref = NN_ref(input)\n",
    "        t2.append(time.time() + last_time); last_time = -time.time()\n",
    "        \n",
    "        y_pred_samples, kl = gp_sample_and_estimate_kl(torch.stack(outputs, 0),\n",
    "            y_preds_ref, 256, 1e-4, W_var, b_var)\n",
    "        y_pred_samples = y_pred_samples[:, :, :y.shape[0]].permute(0, 2, 1).flatten(0,1)\n",
    "\n",
    "        loss = loss_fn(y_pred_samples, y.repeat(256, 1))\n",
    "        NNs_hist_train.append(loss.item())\n",
    "        \n",
    "        kl = kl / input.size(0)*data_noise*2\n",
    "        kls.append(kl.item())\n",
    "        \n",
    "        t3.append(time.time() + last_time); last_time = -time.time()\n",
    "        \n",
    "        # run gradient update\n",
    "        optimizer.zero_grad()\n",
    "        ((loss + kl_weight * kl) * n_ensemble).backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        t4.append(time.time() + last_time); last_time = -time.time()\n",
    "        \n",
    "    print(np.mean(t1[-100:]), np.std(t1[-100:]))\n",
    "    print(np.mean(t2[-100:]), np.std(t2[-100:]))\n",
    "    print(np.mean(t3[-100:]), np.std(t3[-100:]))\n",
    "    print(np.mean(t4[-100:]), np.std(t4[-100:]))\n",
    "    print(np.mean(np.array(t1[-100:])+np.array(t2[-100:])+np.array(t3[-100:])+np.array(t4[-100:])), \n",
    "          np.std(np.array(t1[-100:])+np.array(t2[-100:])+np.array(t3[-100:])+np.array(t4[-100:])))\n",
    "\n",
    "    # run predictions\n",
    "    y_preds, y_preds_mu, y_preds_std = fn_predict_ensemble(NNs,x_test)\n",
    "    plot(x_test, y_preds, y_preds_mu, y_preds_std, data_noise,\n",
    "         show_preds=False, show_mu_std=True, show_trains=True, title='degp_{}_l{}_h{}{}'.format(ip, n_layer, n_hidden, '_nottrainanc'))\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "l_rate = 1e-3\n",
    "opt = 'sgd'\n",
    "ip = None #\n",
    "gp_ensemble(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "            W_var, b_var, W1_var, b1_var, W2_var, b2_var, 0, n_hidden, ip=ip, kl_weight=1,\n",
    "            n_ensembles_ref=1, n_hidden_ref=64)\n",
    "gp_ensemble(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "            W_var, b_var, W1_var, b1_var, W2_var, b2_var, 1, n_hidden, ip=ip, kl_weight=1,\n",
    "            n_ensembles_ref=10, n_hidden_ref=64)\n",
    "gp_ensemble(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "            W_var, b_var, W1_var, b1_var, W2_var/2., b2_var, 2, n_hidden*2, ip=ip, kl_weight=1,\n",
    "            n_ensembles_ref=10, n_hidden_ref=128)\n",
    "gp_ensemble(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "            W_var, b_var, W1_var, b1_var, W2_var/4., b2_var, 3, n_hidden*4, ip=ip, kl_weight=1,\n",
    "            n_ensembles_ref=10, n_hidden_ref=256)\n",
    "\n",
    "ip = 'uniform'\n",
    "gp_ensemble(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "            W_var, b_var, W1_var, b1_var, W2_var, b2_var, 0, n_hidden, ip=ip, kl_weight=1,\n",
    "            n_ensembles_ref=1, n_hidden_ref=64)\n",
    "gp_ensemble(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "            W_var, b_var, W1_var, b1_var, W2_var, b2_var, 1, n_hidden, ip=ip, kl_weight=1,\n",
    "            n_ensembles_ref=10, n_hidden_ref=64)\n",
    "gp_ensemble(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "            W_var, b_var, W1_var, b1_var, W2_var/2., b2_var, 2, n_hidden*2, ip=ip, kl_weight=1,\n",
    "            n_ensembles_ref=10, n_hidden_ref=128)\n",
    "gp_ensemble(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "            W_var, b_var, W1_var, b1_var, W2_var/4., b2_var, 3, n_hidden*4, ip=ip, kl_weight=1,\n",
    "            n_ensembles_ref=10, n_hidden_ref=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:29<00:00, 33.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002515556812286377 0.00042943912705453664\n",
      "0.011570332050323486 0.0018231084036171297\n",
      "0.014425351619720458 0.00296968181153318\n",
      "0.02851124048233032 0.004872009705865266\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=64, out_features=1, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:44<00:00, 22.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006384060382843017 0.0012717318090779963\n",
      "0.017777044773101807 0.0033267243306367785\n",
      "0.023979873657226564 0.006301936546593667\n",
      "0.048140978813171385 0.01018761749959668\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.0, inplace=False)\n",
      "  (6): Linear(in_features=128, out_features=1, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:55<00:00,  8.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02233818292617798 0.029393050173881455\n",
      "0.025958306789398193 0.004532217194166111\n",
      "0.06743485689163208 0.046118906040921975\n",
      "0.11573134660720825 0.05405702935010537\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.0, inplace=False)\n",
      "  (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Dropout(p=0.0, inplace=False)\n",
      "  (9): Linear(in_features=256, out_features=1, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:31<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026557199954986573 0.024655306691486537\n",
      "0.06299645185470581 0.03547011869798585\n",
      "0.11892693996429443 0.027842216311070356\n",
      "0.2084805917739868 0.05292156104039831\n"
     ]
    }
   ],
   "source": [
    "def anchored_ensemble(x_train, y_train, x_test, reg, data_noise, n_ensemble, activation_fn, \n",
    "                      W1_var, b1_var, W2_var, b2_var, n_layer, n_hidden):\n",
    "    x = torch.tensor(x_train).float()\n",
    "    y = torch.tensor(y_train).float()\n",
    "    # set up loss\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    # create the NNs\n",
    "    anchor_NNs=[]\n",
    "    for m in range(n_ensemble):\n",
    "        if n_layer <= 0:\n",
    "            anchor_NNs.append(torch.nn.Sequential(torch.nn.Linear(1, 1)))\n",
    "        else:\n",
    "            anchor_NNs.append(fn_make_NN(activation_fn, n_layer, n_hidden))\n",
    "        init_NN(anchor_NNs[-1], W1_var, b1_var, W2_var, b2_var)\n",
    "        for p in anchor_NNs[-1].parameters():\n",
    "            p.requires_grad_(False)\n",
    "    \n",
    "    NNs, params = [], []\n",
    "    for m in range(n_ensemble):\n",
    "        if n_layer <= 0:\n",
    "            NNs.append(torch.nn.Sequential(torch.nn.Linear(1, 1)))\n",
    "        else:\n",
    "            NNs.append(fn_make_NN(activation_fn, n_layer, n_hidden))\n",
    "        init_NN(NNs[-1], W1_var, b1_var, W2_var, b2_var)\n",
    "        for p in NNs[m].parameters():\n",
    "            params.append(p)\n",
    "    if opt == 'adam':\n",
    "        optimizer = torch.optim.Adam(params, lr=l_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(params, lr=l_rate, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    \n",
    "    print(NNs[0])\n",
    "    \n",
    "    # plot priors\n",
    "    y_preds, y_preds_mu, y_preds_std = fn_predict_ensemble(NNs,x_test)\n",
    "    plot(x_test, y_preds, None, None, data_noise,\n",
    "         show_preds=True, show_mu_std=False, show_trains=False)\n",
    "    \n",
    "    # do training\n",
    "    NNs_hist_train=[[] for _ in range(n_ensemble)]\n",
    "    t1, t2, t3 = [], [], []\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        losses, l2s, y_preds = [], [], []\n",
    "        \n",
    "        last_time = -time.time()\n",
    "        for m in range(n_ensemble):\n",
    "            y_preds.append(NNs[m](x))\n",
    "        t1.append(time.time() + last_time); last_time = -time.time()\n",
    "            \n",
    "        for m in range(n_ensemble):\n",
    "            losses.append(loss_fn(y_preds[m], y))\n",
    "            NNs_hist_train[m].append(losses[m].item())\n",
    "        \n",
    "        for m in range(n_ensemble):\n",
    "            # set up reg loss\n",
    "            l2 = 0\n",
    "            if reg == 'anc':\n",
    "                l2 += ((NNs[m][0].weight - anchor_NNs[m][0].weight)**2).sum() / W1_var\n",
    "                l2 += ((NNs[m][0].bias - anchor_NNs[m][0].bias)**2).sum() / b1_var\n",
    "                for p1, p2 in zip(list(NNs[m][1:].parameters()), list(anchor_NNs[m][1:].parameters())):\n",
    "                    l2 += ((p1 - p2)**2).sum() / (b2_var if p1.dim() == 1 else W2_var)\n",
    "            elif reg == 'reg':\n",
    "                l2 += (NNs[m][0].weight**2).sum() / W1_var\n",
    "                l2 += (NNs[m][0].bias**2).sum() / b1_var\n",
    "                for p in NNs[m][1:].parameters():\n",
    "                    l2 += (p**2).sum() / (b2_var if p.dim() == 1 else W2_var)\n",
    "            elif reg == 'free':\n",
    "                # do nothing\n",
    "                l2 += 0.0\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            l2 = l2 / x.size(0) * data_noise / 4. # to avoid over-regularization in practice\n",
    "            l2s.append(l2)\n",
    "        \n",
    "        t2.append(time.time() + last_time); last_time = -time.time()\n",
    "\n",
    "        # run gradient update\n",
    "        optimizer.zero_grad()\n",
    "        (sum(losses) + sum(l2s)).backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        t3.append(time.time() + last_time); last_time = -time.time()\n",
    "    \n",
    "    print(np.mean(t1[-100:]), np.std(t1[-100:]))\n",
    "    print(np.mean(t2[-100:]), np.std(t2[-100:]))\n",
    "    print(np.mean(t3[-100:]), np.std(t3[-100:]))\n",
    "    print(np.mean(np.array(t1[-100:])+np.array(t2[-100:])+np.array(t3[-100:])), \n",
    "          np.std(np.array(t1[-100:])+np.array(t2[-100:])+np.array(t3[-100:])))\n",
    "\n",
    "    # run predictions\n",
    "    y_preds, y_preds_mu, y_preds_std = fn_predict_ensemble(NNs,x_test)\n",
    "    plot(x_test, y_preds, y_preds_mu, y_preds_std, data_noise, \n",
    "         show_preds=True, show_mu_std=False, show_trains=True, title=reg+'_weight_l{}_h{}_pred'.format(n_layer, n_hidden))\n",
    "    plot(x_test, y_preds, y_preds_mu, y_preds_std, data_noise, \n",
    "         show_preds=False, show_mu_std=True, show_trains=True, title=reg+'_weight_l{}_h{}'.format(n_layer, n_hidden))\n",
    "    \n",
    "# anchored_ensemble(x_train, y_train, x_test, 'anc', data_noise, n_ensemble, activation_fn, \n",
    "#                   W1_var, b1_var, W2_var, b2_var, n_layer, n_hidden)\n",
    "# anchored_ensemble(x_train, y_train, x_test, 'reg', data_noise, n_ensemble, activation_fn, \n",
    "#                   W1_var, b1_var, W2_var, b2_var, n_layer, n_hidden)\n",
    "\n",
    "anchored_ensemble(x_train, y_train, x_test, 'free', data_noise, n_ensemble, activation_fn, \n",
    "                  W1_var, b1_var, W2_var, b2_var, 0, n_hidden)\n",
    "anchored_ensemble(x_train, y_train, x_test, 'free', data_noise, n_ensemble, activation_fn, \n",
    "                  W1_var, b1_var, W2_var, b2_var, 1, n_hidden)\n",
    "anchored_ensemble(x_train, y_train, x_test, 'free', data_noise, n_ensemble, activation_fn, \n",
    "                  W1_var, b1_var, W2_var/2., b2_var, 2, n_hidden*2)\n",
    "anchored_ensemble(x_train, y_train, x_test, 'free', data_noise, n_ensemble, activation_fn, \n",
    "                  W1_var, b1_var, W2_var/4., b2_var, 3, n_hidden*4)\n",
    "\n",
    "anchored_ensemble(x_train, y_train, x_test, 'reg', data_noise, n_ensemble, activation_fn, \n",
    "                  W1_var, b1_var, W2_var, b2_var, 0, n_hidden)\n",
    "anchored_ensemble(x_train, y_train, x_test, 'reg', data_noise, n_ensemble, activation_fn, \n",
    "                  W1_var, b1_var, W2_var, b2_var, 1, n_hidden)\n",
    "anchored_ensemble(x_train, y_train, x_test, 'reg', data_noise, n_ensemble, activation_fn, \n",
    "                  W1_var, b1_var, W2_var/2., b2_var, 2, n_hidden*2)\n",
    "anchored_ensemble(x_train, y_train, x_test, 'reg', data_noise, n_ensemble, activation_fn, \n",
    "                  W1_var, b1_var, W2_var/4., b2_var, 3, n_hidden*4)\n",
    "\n",
    "\n",
    "# anchored_ensemble(x_train, y_train, x_test, 'anc', data_noise, n_ensemble, activation_fn, \n",
    "#                   W1_var, b1_var, W2_var/4., b2_var, 3, n_hidden*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marg_log_like [[-8.42169435]]\n"
     ]
    }
   ],
   "source": [
    "def rbf_kernel(x1, x2, sigma=1., l=0.1):\n",
    "    if x2 is None:\n",
    "        x2 = x1\n",
    "    if isinstance(x1, np.ndarray):\n",
    "        return np.exp(((x1[:, None, :] - x2[None, :, :])**2).sum(-1)/(-2)/(l**2)) * (sigma**2)\n",
    "    else:\n",
    "        return torch.exp(((x1[:, None, :] - x2[None, :, :])**2).sum(-1)/(-2)/(l**2)) * (sigma**2)\n",
    "    \n",
    "def gp(x_train, y_train, x_test, data_noise, sigma=1., l=0.1):\n",
    "    \n",
    "    cov_dd = rbf_kernel(x_train, None, sigma, l) + np.identity(x_train.shape[0])*data_noise\n",
    "    cov_xd = rbf_kernel(x_test, x_train, sigma, l)\n",
    "    cov_xx = rbf_kernel(x_test, None, sigma, l)\n",
    "    \n",
    "    L = np.linalg.cholesky(cov_dd)\n",
    "    alpha = np.linalg.solve(L.T,np.linalg.solve(L,y_train))\n",
    "    y_pred_mu = np.matmul(cov_xd,alpha)\n",
    "    v = np.linalg.solve(L,cov_xd.T)\n",
    "    cov_pred = cov_xx - np.matmul(v.T,v)\n",
    "\n",
    "    y_pred_var = np.atleast_2d(np.diag(cov_pred) + data_noise).T\n",
    "    y_pred_std = np.sqrt(y_pred_var)\n",
    "    \n",
    "    marg_log_like = - np.matmul(y_train.T,alpha)/2 - np.sum(np.log(np.diag(L))) - x_train.shape[0]*np.log(2*np.pi)/2\n",
    "    print('marg_log_like', marg_log_like)\n",
    "    \n",
    "    # visualize posteriors\n",
    "    plot(x_test, None, y_pred_mu, y_pred_std, data_noise, not_apply_data_noise=True,\n",
    "         show_preds=False, show_mu_std=True, show_trains=True, title='gp')\n",
    "\n",
    "gp(x_train, y_train, x_test, data_noise, sigma=1, l=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:09<00:00, 101.73it/s]\n"
     ]
    }
   ],
   "source": [
    "def mc_dropout(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "               W1_var, b1_var, W2_var, b2_var, n_layer, n_hidden, dropout=0.):\n",
    "    x = torch.tensor(x_train).float()\n",
    "    y = torch.tensor(y_train).float()\n",
    "    \n",
    "    # create the NNs\n",
    "    NN=fn_make_NN(activation_fn, n_layer, n_hidden, dropout=dropout)\n",
    "    init_NN(NN, W1_var, b1_var, W2_var, b2_var)\n",
    "\n",
    "    # do training\n",
    "    NN_hist_train=[]\n",
    "    # set up loss\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    params = []\n",
    "    for p in NN.parameters():\n",
    "        if p.requires_grad:\n",
    "            params.append(p)\n",
    "    optimizer = torch.optim.Adam(params, lr=l_rate)\n",
    "    \n",
    "    for _ in tqdm(range(epochs)):\n",
    "        loss = loss_fn(NN(x), y)\n",
    "        NN_hist_train.append(loss.item())\n",
    "        # run gradient update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # run predictions\n",
    "    with torch.no_grad():\n",
    "        y_preds = torch.stack([NN(torch.tensor(x_test).float()) for _ in range(n_ensemble)], 0).data.cpu().numpy()\n",
    "    y_preds_mu = np.mean(y_preds,axis=0)\n",
    "    y_preds_std = np.std(y_preds,axis=0)\n",
    "    plot(x_test, None, y_preds_mu, y_preds_std, data_noise,\n",
    "         show_preds=False, show_mu_std=True, show_trains=True, title='mcd_h{}'.format(n_hidden))\n",
    "\n",
    "mc_dropout(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "           W1_var, b1_var, W2_var/4., b2_var, 3, n_hidden*4, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_inf_samples = 2000 # number samples to take during inference\n",
    "n_pred_samples = 200 # number samples to take during prediction\n",
    "vi_steps = 200000 # number optimisation steps to run for VI \n",
    "\n",
    "def build_model_pm(total_size, ann_input, ann_output, data_noise, activation_fn, \n",
    "                   W1_var, b1_var, W2_var, b2_var, n_layer, n_hidden):\n",
    "    with pm.Model() as model:\n",
    "        for i in range(n_layer+1):\n",
    "            n_in = 1 if i == 0 else n_hidden\n",
    "            n_out = 1 if i == n_layer else n_hidden\n",
    "            W_var = W1_var if i == 0 else W2_var\n",
    "            b_var = b1_var if i == 0 else b2_var\n",
    "            \n",
    "            init_w = np.random.normal(loc=0, scale=np.sqrt(W_var), size=[n_in, n_out]).astype(floatX)\n",
    "            init_b = np.random.normal(loc=0, scale=np.sqrt(b_var), size=[n_out]).astype(floatX)\n",
    "            weights_w = pm.Normal('w_{}'.format(i), 0, sd=np.sqrt(W_var), shape=(n_in, n_out), testval=init_w)\n",
    "            weights_b = pm.Normal('b_{}'.format(i), 0, sd=np.sqrt(b_var), shape=(n_out), testval=init_b) if i<n_layer else 0\n",
    "\n",
    "            act_pre = pm.math.dot(ann_input if i == 0 else act_out, weights_w) + weights_b\n",
    "            if i < n_layer:\n",
    "                if activation_fn == 'relu':\n",
    "                    act_out = pm.math.maximum(act_pre, 0)\n",
    "                elif activation_fn =='erf':\n",
    "                    act_out = pm.math.erf(act_pre)\n",
    "\n",
    "        out = pm.Normal('out', act_pre,sd=np.sqrt(data_noise),\n",
    "                        observed=ann_output,\n",
    "                        total_size=total_size)\n",
    "    return model, out\n",
    "\n",
    "def hmc_vi(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "           W1_var, b1_var, W2_var, b2_var, n_layer, n_hidden, is_hmc=True):\n",
    "    ann_input = theano.shared(x_train)\n",
    "    ann_output = theano.shared(y_train)\n",
    "    BNN, out = build_model_pm(len(y_train), ann_input, ann_output, data_noise, activation_fn, \n",
    "                                W1_var, b1_var, W2_var, b2_var, n_layer, n_hidden)\n",
    "    print(BNN)\n",
    "    # run inference\n",
    "    if is_hmc:\n",
    "        step = pm.HamiltonianMC(path_length=0.5, adapt_step_size=True, step_scale=0.04,\n",
    "            gamma=0.05, k=0.9, t0=1, target_accept=0.95, model=BNN)\n",
    "        trace = pm.sample(n_inf_samples, step=step, model=BNN, chains=1, n_jobs=1, tune=300)\n",
    "        # reduce path_length if failing - 5.0 is ok with cos_lin data\n",
    "    else:\n",
    "        # https://docs.pymc.io/notebooks/bayesian_neural_network_advi.html\n",
    "        inference = pm.ADVI(model=BNN)\n",
    "        approx = pm.fit(n=vi_steps, method=inference, model=BNN)\n",
    "        trace = approx.sample(draws=n_inf_samples)\n",
    "\n",
    "        if True:\n",
    "            fig = plt.figure(figsize=(8, 4))\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.plot(-inference.hist, label='new ADVI', alpha=.3)\n",
    "            ax.plot(approx.hist, label='old ADVI', alpha=.3)\n",
    "            ax.set_ylabel('ELBO');\n",
    "            ax.set_xlabel('iteration');\n",
    "            fig.show()\n",
    "    \n",
    "    # make predictions\n",
    "    ann_input.set_value(x_test.astype('float32'))\n",
    "    ann_output.set_value(x_test.astype('float32'))\n",
    "\n",
    "    ppc = pm.sample_ppc(trace, model=BNN, samples=n_pred_samples) # this does new set of preds per point\n",
    "    y_preds = ppc['out']\n",
    "    y_preds_mu = y_preds.mean(axis=0)\n",
    "    y_preds_std = y_preds.std(axis=0)\n",
    "\n",
    "    plot(x_test, None, y_preds_mu, y_preds_std, data_noise, not_apply_data_noise=True,\n",
    "         show_preds=False, show_mu_std=True, show_trains=True, title='vi_h{}'.format(n_hidden) if not is_hmc else 'hmc_h{}'.format(n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0906bc6384d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m hmc_vi(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n\u001b[0;32m----> 2\u001b[0;31m        W1_var, b1_var, W2_var, b2_var, n_layer, n_hidden, is_hmc=True)\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'n_layer' is not defined"
     ]
    }
   ],
   "source": [
    "hmc_vi(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "       W1_var, b1_var, W2_var, b2_var, n_layer, n_hidden, is_hmc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_vi(x_train, y_train, x_test, data_noise, n_ensemble, activation_fn, \n",
    "       W1_var, b1_var, W2_var, b2_var, n_layer, n_hidden, is_hmc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
